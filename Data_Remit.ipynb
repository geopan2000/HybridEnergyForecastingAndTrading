{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa0045e-97e7-4a55-b5ff-f79dfc3e7796",
   "metadata": {},
   "source": [
    "# Hybrid Renewable Energy Forecasting and Trading Competition\n",
    "Author: George Panagiotou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e9f71-138a-40b6-a1fe-fe78d0ef4e15",
   "metadata": {},
   "source": [
    "# Hornsea 1 Wind Farm Data Processing\n",
    "\n",
    "## Overview\n",
    "\n",
    "The main function of this notebook is to read the availability messages provided for the Hornsea 1 wind farm by REMIT (https://bmrs.elexon.co.uk/remit).\n",
    "\n",
    "The Hornsea 1 wind farm is comprised of three balancing mechanism units: T_HOWAO-1, T_HOWAO-2, and T_HOWAO-3.\n",
    "\n",
    "The data are provided in JSON format, and the challenge here is to transform them into a static DataFrame while combining them with the energy data provided by the competition.\n",
    "\n",
    "## REMIT Message Format\n",
    "\n",
    "Every REMIT message has the following format: (Example)\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": 64337,\n",
    "    \"dataset\": \"REMIT\",\n",
    "    \"mrid\": \"11XDONG-PT-----2-NGET-RMT-00001018\",\n",
    "    \"revisionNumber\": 2,\n",
    "    \"publishTime\": \"2020-11-26T13:53:00Z\",\n",
    "    \"createdTime\": \"2020-11-26T13:53:00Z\",\n",
    "    \"messageType\": \"UnavailabilitiesOfElectricityFacilities\",\n",
    "    \"messageHeading\": \"REMIT Information\",\n",
    "    \"eventType\": \"Production unavailability\",\n",
    "    \"unavailabilityType\": \"Planned\",\n",
    "    \"participantId\": \"DONG013\",\n",
    "    \"registrationCode\": \"11XDONG-PT-----2\",\n",
    "    \"assetId\": \"T_HOWAO-1\",\n",
    "    \"assetType\": \"Production\",\n",
    "    \"affectedUnit\": \"HOWAO-1\",\n",
    "    \"affectedUnitEIC\": \"48W00000HOWAO-1M\",\n",
    "    \"affectedArea\": \"B7\",\n",
    "    \"biddingZone\": \"10YGB----------A\",\n",
    "    \"fuelType\": \"Wind Offshore\",\n",
    "    \"normalCapacity\": 400,\n",
    "    \"availableCapacity\": 0,\n",
    "    \"unavailableCapacity\": 400,\n",
    "    \"eventStatus\": \"Active\",\n",
    "    \"eventStartTime\": \"2020-11-26T09:00:00Z\",\n",
    "    \"eventEndTime\": \"2020-11-26T18:00:00Z\",\n",
    "    \"cause\": \"Planned Outage\",\n",
    "    \"relatedInformation\": \"HOW01 Z11 Dry run interlink test\",\n",
    "    \"outageProfile\": [\n",
    "      {\n",
    "        \"startTime\": \"2020-11-26T09:00:00Z\",\n",
    "        \"endTime\": \"2020-11-26T18:00:00Z\",\n",
    "        \"capacity\": 0\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f8033",
   "metadata": {},
   "source": [
    "## Key Information\n",
    "The information that we care about includes:\n",
    "\n",
    "Publish time,\n",
    "End time,\n",
    "Available capacity\n",
    "\n",
    "## Objective\n",
    "The main objective is to create three new training features (one for each balancing mechanism) and merge them with the energy data. Additionally, we perform filtering based on the publish time of the messages.\n",
    "\n",
    "For the training set, we used all available information regardless of the publish time. However, for the test set, we must use only the information based on the publish time. Sometimes, messages contain unavailabilities that occurred before the publish time, which in real life, we would not be able to use. Additionally, some messages contain information about events occurring during the day, so if the message for the day-ahead market times came after the submission time (9:20), we are not able to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d72678-cb73-49d4-9aeb-2dd667574537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two CSV files\n",
    "#The first set contains the energy data from 2020/09/20 to 2024/01/18\n",
    "energy_data1 = pd.read_csv(\"data/HEFTdata/Energy_Data_20200920_20240118.csv\")\n",
    "#The first set contains the energy data from 2024/01/19 to 2024/05/19\n",
    "energy_data2 = pd.read_csv(\"data/HEFTdata/Energy_Data_20240119_20240519.csv\")\n",
    "# Combine the DataFrames\n",
    "energy_data = pd.concat([energy_data1, energy_data2])\n",
    "energy_data.to_hdf('data/combined/energy_data_20200920_20240519.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94202629-eb02-4a33-9083-e1a10fdfbd03",
   "metadata": {},
   "source": [
    "## Combine all the Availability messages of T-HAWAO-1\n",
    "\n",
    "From the link above the maximum limit of offline data that you can download is 1 year and thus we have to combine 4 sets of data to allign with the data of HEFTcompetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66dad8a-760d-49c9-8c7f-d7b73a8817bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON data saved to data/T_Hawao/T_Hawao-1/combined_T_HAWAO-1.json\n"
     ]
    }
   ],
   "source": [
    "# List of JSON file paths\n",
    "# These data have been downloaded \n",
    "json_files = [\n",
    "    'data/T_Hawao/T_Hawao-1/T_HAWAO-1-2020-09-20-2021-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-1/T_HAWAO-1-2021-05-20-2022-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-1/T_HAWAO-1-2022-05-20-2023-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-1/T_HOWAO-1-2023-05-20-2024-05-20.json'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Read and combine each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "# Remove duplicates based on 'id' field\n",
    "unique_data = {entry['id']: entry for entry in combined_data}\n",
    "combined_unique_data = list(unique_data.values())\n",
    "\n",
    "# Save the combined data into a new JSON file\n",
    "output_file_path = 'data/T_Hawao/T_Hawao-1/combined_T_HAWAO-1.json'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(combined_unique_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined JSON data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ab50d-d770-45ea-8b79-9b970373b9bf",
   "metadata": {},
   "source": [
    "## Combine all the Availability messages of T-HAWAO-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0b3295-9547-4bcd-857f-ca4d082a14ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON data saved to data/T_Hawao/T_Hawao-2/combined_T_HAWAO-2.json\n"
     ]
    }
   ],
   "source": [
    "# List of JSON file paths\n",
    "json_files = [\n",
    "    'data/T_Hawao/T_Hawao-2/T_HAWAO-2-2020-09-20-2021-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-2/T_HAWAO-2-2021-05-20-2022-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-2/T_HAWAO-2-2022-05-20-2023-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-2/T_HOWAO-2-2023-05-20-2024-05-20.json'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Read and combine each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "# Remove duplicates based on 'id' field\n",
    "unique_data = {entry['id']: entry for entry in combined_data}\n",
    "combined_unique_data = list(unique_data.values())\n",
    "\n",
    "# Save the combined data into a new JSON file\n",
    "output_file_path = 'data/T_Hawao/T_Hawao-2/combined_T_HAWAO-2.json'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(combined_unique_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined JSON data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0e47a-4d8c-4172-a400-25903bf794e8",
   "metadata": {},
   "source": [
    "## Combine all the Availability messages of T-HAWAO-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caba5e2f-9028-42f9-9ea4-4070d123bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined JSON data saved to data/T_Hawao/T_Hawao-3/combined_T_HAWAO-3.json\n"
     ]
    }
   ],
   "source": [
    "# List of JSON file paths\n",
    "json_files = [\n",
    "    'data/T_Hawao/T_Hawao-3/T_HAWAO-3-2020-09-20-2021-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-3/T_HAWAO-3-2021-05-20-2022-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-3/T_HAWAO-3-2022-05-20-2023-05-21.json',\n",
    "    'data/T_Hawao/T_Hawao-3/T_HOWAO-3-2023-05-20-2024-05-20.json'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Read and combine each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "# Remove duplicates based on 'id' field\n",
    "unique_data = {entry['id']: entry for entry in combined_data}\n",
    "combined_unique_data = list(unique_data.values())\n",
    "\n",
    "# Save the combined data into a new JSON file\n",
    "output_file_path = 'data/T_Hawao/T_Hawao-3/combined_T_HAWAO-3.json'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(combined_unique_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined JSON data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db534b1-a532-4609-8d63-1df6b62799e2",
   "metadata": {},
   "source": [
    "## Create new features from Remit data:\n",
    "We are going to create 3 new features: Availability1 Availability2 and Availability3. Based on this, we will train the model, with extra information.\n",
    "\n",
    "This extra information will help the training by knowing if the Hornsea1 is not able to produce its full capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46559921-0a7d-4a97-a973-28281013c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated energy data saved to data/combined/train_energy_data_20200920_20240519.h5\n"
     ]
    }
   ],
   "source": [
    "# Load the HDF5 file\n",
    "energy_data = pd.read_hdf('data/combined/energy_data_20200920_20240519.h5', 'df')\n",
    "energy_data[\"dtm\"] = pd.to_datetime(energy_data[\"dtm\"])\n",
    "\n",
    "# Initialize the new features to the normal capacity, which is 400\n",
    "energy_data['Availability1'] = 400\n",
    "energy_data['Availability2'] = 400\n",
    "energy_data['Availability3'] = 400\n",
    "\n",
    "# List of JSON file paths and corresponding availability columns\n",
    "json_files = {\n",
    "    'data/T_Hawao/T_Hawao-1/combined_T_HAWAO-1.json': 'Availability1',\n",
    "    'data/T_Hawao/T_Hawao-2/combined_T_HAWAO-2.json': 'Availability2',\n",
    "    'data/T_Hawao/T_Hawao-3/combined_T_HAWAO-3.json': 'Availability3'\n",
    "}\n",
    "\n",
    "# Function to update the Availability feature based on the outage profiles\n",
    "def update_availability(row, outage_profiles, availability_column):\n",
    "    for profile in outage_profiles:\n",
    "        start_time = pd.to_datetime(profile['startTime'])\n",
    "        end_time = pd.to_datetime(profile['endTime'])\n",
    "        if start_time <= row['dtm'] < end_time:\n",
    "            return profile['capacity']\n",
    "    return row[availability_column]\n",
    "\n",
    "# Iterate through each JSON file and update the corresponding Availability feature\n",
    "for json_file, availability_column in json_files.items():\n",
    "    with open(json_file, 'r') as file:\n",
    "        outage_data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the outage profiles\n",
    "    outage_entries = []\n",
    "    for entry in outage_data:\n",
    "        for profile in entry['outageProfile']:\n",
    "            outage_entries.append({\n",
    "                'startTime': pd.to_datetime(profile['startTime']),\n",
    "                'endTime': pd.to_datetime(profile['endTime']),\n",
    "                'capacity': profile['capacity']\n",
    "            })\n",
    "    \n",
    "    outage_df = pd.DataFrame(outage_entries)\n",
    "    \n",
    "    # Update the Availability feature for each outage profile\n",
    "    for _, row in outage_df.iterrows():\n",
    "        mask = (energy_data['dtm'] >= row['startTime']) & (energy_data['dtm'] < row['endTime'])\n",
    "        energy_data.loc[mask, availability_column] = row['capacity']\n",
    "\n",
    "# Save the updated data back to an HDF5 file\n",
    "output_file_path = 'data/combined/train_energy_data_20200920_20240519.h5'\n",
    "energy_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "\n",
    "print(f\"Updated energy data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778021a-95b0-4b08-838e-23bdf5757bfc",
   "metadata": {},
   "source": [
    "## Train energy data\n",
    "\n",
    "The test data have a slightly different approach\n",
    "\n",
    "If the publish time is before 9 AM:  \n",
    "Apply changes for outage profiles that affect the period from 22:00 of the same day onwards.  \n",
    "If the publish time is after 9 AM:  \n",
    "Apply changes for outage profiles that affect the period from 22:00 of the next day onwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f3559d-541c-4e8b-a108-6474d62ff596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated energy data saved to data/combined/test_energy_data_20200920_20240519.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Provided function to get day-ahead market times\n",
    "def day_ahead_market_times(today_date=pd.to_datetime('today')):\n",
    "    tomorrow_date = today_date + pd.Timedelta(1, unit=\"day\")\n",
    "    DA_Market = [pd.Timestamp(datetime.datetime(today_date.year, today_date.month, today_date.day, 23, 0, 0),\n",
    "                              tz=\"Europe/London\"),\n",
    "                 pd.Timestamp(datetime.datetime(tomorrow_date.year, tomorrow_date.month, tomorrow_date.day, 22, 30, 0),\n",
    "                              tz=\"Europe/London\")]\n",
    "\n",
    "    DA_Market = pd.date_range(start=DA_Market[0], end=DA_Market[1], freq=pd.Timedelta(30, unit=\"minute\"))\n",
    "\n",
    "    return DA_Market\n",
    "\n",
    "# Load the HDF5 file\n",
    "energy_data = pd.read_hdf('data/combined/energy_data_20200920_20240519.h5', 'df')\n",
    "energy_data[\"dtm\"] = pd.to_datetime(energy_data[\"dtm\"]).dt.tz_convert('Europe/London')\n",
    "\n",
    "# Initialize the new features to the normal capacity, which is 400\n",
    "energy_data['Availability1'] = 400\n",
    "energy_data['Availability2'] = 400\n",
    "energy_data['Availability3'] = 400\n",
    "\n",
    "# List of JSON file paths and corresponding availability columns\n",
    "json_files = {\n",
    "    'data/T_Hawao/T_Hawao-1/combined_T_HAWAO-1.json': 'Availability1',\n",
    "    'data/T_Hawao/T_Hawao-2/combined_T_HAWAO-2.json': 'Availability2',\n",
    "    'data/T_Hawao/T_Hawao-3/combined_T_HAWAO-3.json': 'Availability3'\n",
    "}\n",
    "\n",
    "# Function to get the day-ahead market times\n",
    "def get_day_ahead_hours(reference_time):\n",
    "    return day_ahead_market_times(reference_time)\n",
    "\n",
    "# Function to apply outage profiles based on message publish time and 9 AM cutoff\n",
    "def apply_outage_profiles(energy_data, outage_data, availability_column):\n",
    "    for entry in outage_data:\n",
    "        publish_time = pd.to_datetime(entry['publishTime']).tz_convert('Europe/London')\n",
    "        \n",
    "        # Determine the start time for the forecast window\n",
    "        if publish_time.time() < pd.to_datetime('09:00:00').time():\n",
    "            forecast_start = publish_time.normalize() + pd.Timedelta(hours=22)  # 22:00 of the same day\n",
    "        else:\n",
    "            forecast_start = publish_time.normalize() + pd.Timedelta(days=1, hours=22)  # 22:00 of the next day\n",
    "\n",
    "        # Apply the outage profiles\n",
    "        for profile in entry['outageProfile']:\n",
    "            start_time = pd.to_datetime(profile['startTime']).tz_convert('Europe/London')\n",
    "            end_time = pd.to_datetime(profile['endTime']).tz_convert('Europe/London')\n",
    "            capacity = profile['capacity']\n",
    "            \n",
    "            # Apply profiles that affect the period from forecast_start onwards\n",
    "            if start_time < forecast_start and end_time > forecast_start:\n",
    "                start_time = forecast_start\n",
    "\n",
    "            if start_time >= forecast_start or end_time > forecast_start:\n",
    "                mask = (energy_data['dtm'] >= start_time) & (energy_data['dtm'] < end_time)\n",
    "                energy_data.loc[mask, availability_column] = capacity\n",
    "\n",
    "# Iterate through each JSON file and update the corresponding Availability feature\n",
    "for json_file, availability_column in json_files.items():\n",
    "    with open(json_file, 'r') as file:\n",
    "        outage_data = json.load(file)\n",
    "    apply_outage_profiles(energy_data, outage_data, availability_column)\n",
    "\n",
    "# Save the updated data back to an HDF5 file\n",
    "output_file_path_hdf = 'data/combined/test_energy_data_20200920_20240519.h5'\n",
    "energy_data.to_hdf(output_file_path_hdf, key='df', mode='w')\n",
    "\n",
    "print(f\"Updated energy data saved to {output_file_path_hdf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
